{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation II\n",
    "## Practical Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "  In this exercise, we will compare $\\ell_1$ and $\\ell_2$ regularization for image denoising using wavelet transform.\n",
    "\n",
    "  One way to view an image is as a function $f(i,j)$ on the pixel position $(i,j)$. Using wavelet transform, we can represent the function (image) as $f(i,j) = \\langle \\mathbf{w}, \\phi(i,j) \\rangle$ where $\\mathbf{w}$ are the linear function weights (wavelet coefficients) and $\\phi(i,j)$ is the feature vector formed by the inverse wavelet transform bases. If we add noise to each pixel and try to learn a predictor (weight vector $\\mathbf{w}$), we have a regression problem.\n",
    "\n",
    "For this problem, it is simpler to think of an image as a vector $\\mathbf{v}$ in $\\mathbb{R}^{n}$, where $n$ is the number of pixels. The $i$-th coefficient of the wavelet transform is then the projection of $\\mathbf{v}$ on the $i$-th basis. Because wavelet is an orthonormal transform, it preserves distance (see background material on linear algebra), i.e. if $\\mathbf{\\bar{w}}$ is an estimate of $\\mathbf{w}$ that gives an estimate $\\mathbf{\\bar{v}}$ of $\\mathbf{v}$, then $\\|\\mathbf{v}-\\mathbf{\\bar{v}}\\|= \\|\\mathbf{w}-\\mathbf{\\bar{w}}\\|$ or $\\sum_{i} (v_i-\\bar{v}_i)^2 = \\sum_{i} (w_i-\\bar{w}_i)^2$. This makes the implementing $\\ell_1$ and $\\ell_2$ regularization easy.\n",
    "\n",
    "For $\\ell_2$ regularization, $\\sum_{i} (v_i-\\bar{v}_i)^2 + \\lambda \\sum_i \\bar{w}_i^2 = \\sum_{i} (w_i-\\bar{w}_i)^2 + \\lambda \\sum_i \\bar{w}_i^2$. Each coefficient $\\bar{w}_i$ can be optimized independently giving the solution $\\bar{w}_i=\\frac{1}{1+\\lambda}w_i$, or each coefficient is multiplied by a constant factor smaller than 1.\n",
    "\n",
    "For $\\ell_1$ regularization, $\\sum_{i} (v_i-\\bar{v}_i)^2 + \\lambda \\sum_i |\\bar{w}_i| = \\sum_{i} (w_i-\\bar{w}_i)^2 + \\lambda \\sum_i |\\bar{w}_i|$. Each coefficient can again be optimized independently, giving the soft thresholding function as the solution: if $|w_i|$ is smaller than $\\lambda$, set $\\bar{w}_i$ to 0, otherwise set $\\bar{w}_i$ to $sign(w_i)(|w_i|-\\lambda)$, i.e. reduce the size of the coefficient by $\\lambda$ while keeping the same sign.\n",
    "\n",
    "Run the experiment comparing $\\ell_1$ and $\\ell_2$ regularization. Performance in signal processing is commonly measured using signal to noise ratio: $SNR(\\mathbf{p},\\bar{\\mathbf{p}}) = 10\\log_{10} \\frac{\\|\\mathbf{p}\\|^2}{\\|\\mathbf{p}-\\bar{\\mathbf{p}}\\|^2}$ (the larger the better). Try other images such as boat.png, flowers.png and mandrill.png. Try varying the soft threshold and ridge parameters.\n",
    "\n",
    "***Archipelago:*** Which regularization method performs better for wavelet denoising? Why?\n",
    "\n",
    "The code is modified from ** Numerical Tours in Python ** http://www.numerical-tours.com/python/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from nt_toolbox.general import *\n",
    "from nt_toolbox.signal import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "# Load image\n",
    "n = 512\n",
    "name = 'nt_toolbox/data/boat.png'\n",
    "f0 = load_image(name, n)\n",
    "imageplot(f0, 'Original image')\n",
    "show()\n",
    "\n",
    "# Add noise\n",
    "sigma = .08\n",
    "f = f0 + sigma*random.standard_normal(f0.shape)\n",
    "imageplot(clamp(f), 'Noisy, SNR=' + str(snr(f0,f)) )\n",
    "show()\n",
    "\n",
    "# Do wavelet tranform\n",
    "h = [0, .482962913145, .836516303738, .224143868042, -.129409522551]\n",
    "h = h/norm(h)\n",
    "Jmin = 2\n",
    "a = perform_wavortho_transf(f,Jmin,+1,h)\n",
    "plot_wavelet(a,Jmin);\n",
    "print \"Wavelet Coefficients\"\n",
    "show()\n",
    "\n",
    "# Soft thresholding, equivalent to l_1 regularization\n",
    "def thresh_soft(u,t):return maximum(1-t/abs(u), 0)*u\n",
    "alpha = linspace(-3,3,1000)\n",
    "plot(alpha, thresh_soft(alpha,1))\n",
    "axis('equal');\n",
    "print \"Soft Thresholding Function\"\n",
    "show()\n",
    "T = 1.4*sigma\n",
    "aT = thresh_soft(a,T)\n",
    "fSoft = perform_wavortho_transf(aT,Jmin,-1,h)\n",
    "imageplot(clamp(fSoft), 'Soft, SNR=' + str(snr(f0,fSoft)) )\n",
    "show()\n",
    "\n",
    "# Ridge regression, equivalent to l_2 regularization\n",
    "aR = 0.98*a\n",
    "fRidge = perform_wavortho_transf(aR,Jmin,-1,h)\n",
    "imageplot(clamp(fRidge), 'Ridge, SNR=' + str(snr(f0,fRidge)) )\n",
    "show()\n",
    "\n",
    "# Mean squared error and fraction of features removed\n",
    "mseR = mean_squared_error(f0,fRidge)\n",
    "mseS = mean_squared_error(f0,fSoft)\n",
    "print \"MSE soft thresholding: \" + \"{0:.4f}\".format(mseS)\n",
    "print \"MSE ridge regression: \" + \"{0:.4f}\".format(mseR)\n",
    "softFrac = sum(aT==0)/(sum(aT!=0)+sum(aT==0))\n",
    "ridgeFrac = sum(aR==0)/(sum(aR!=0)+sum(aR==0))\n",
    "print \"Fraction of zero coeffs in soft thresholding: \" + \"{0:.2f}\".format(softFrac)\n",
    "print \"Fraction of zero coeffs in ridge regression: \" + \"{0:.2f}\".format(ridgeFrac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "Demo on bagging from http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Author: Gilles Louppe <g.louppe@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Settings\n",
    "n_repeat = 50       # Number of iterations for computing expectations\n",
    "n_train = 50        # Size of the training set\n",
    "n_test = 1000       # Size of the test set\n",
    "noise = 0.1         # Standard deviation of the noise\n",
    "np.random.seed(0)\n",
    "\n",
    "# Change this for exploring the bias-variance decomposition of other\n",
    "# estimators. This should work well for estimators with high variance (e.g.,\n",
    "# decision trees or KNN), but poorly for estimators with low variance (e.g.,\n",
    "# linear models).\n",
    "estimators = [(\"Tree\", DecisionTreeRegressor()),\n",
    "              (\"Bagging(Tree)\", BaggingRegressor(DecisionTreeRegressor()))]\n",
    "\n",
    "n_estimators = len(estimators)\n",
    "\n",
    "# Generate data\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "def generate(n_samples, noise, n_repeat=1):\n",
    "    X = np.random.rand(n_samples) * 10 - 5\n",
    "    X = np.sort(X)\n",
    "\n",
    "    if n_repeat == 1:\n",
    "        y = f(X) + np.random.normal(0.0, noise, n_samples)\n",
    "    else:\n",
    "        y = np.zeros((n_samples, n_repeat))\n",
    "\n",
    "        for i in range(n_repeat):\n",
    "            y[:, i] = f(X) + np.random.normal(0.0, noise, n_samples)\n",
    "\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(n_repeat):\n",
    "    X, y = generate(n_samples=n_train, noise=noise)\n",
    "    X_train.append(X)\n",
    "    y_train.append(y)\n",
    "\n",
    "X_test, y_test = generate(n_samples=n_test, noise=noise, n_repeat=n_repeat)\n",
    "\n",
    "# Loop over estimators to compare\n",
    "for n, (name, estimator) in enumerate(estimators):\n",
    "    # Compute predictions\n",
    "    y_predict = np.zeros((n_test, n_repeat))\n",
    "\n",
    "    for i in range(n_repeat):\n",
    "        estimator.fit(X_train[i], y_train[i])\n",
    "        y_predict[:, i] = estimator.predict(X_test)\n",
    "\n",
    "    # Bias^2 + Variance + Noise decomposition of the mean squared error\n",
    "    y_error = np.zeros(n_test)\n",
    "\n",
    "    for i in range(n_repeat):\n",
    "        for j in range(n_repeat):\n",
    "            y_error += (y_test[:, j] - y_predict[:, i]) ** 2\n",
    "\n",
    "    y_error /= (n_repeat * n_repeat)\n",
    "\n",
    "    y_noise = np.var(y_test, axis=1)\n",
    "    y_bias = (f(X_test) - np.mean(y_predict, axis=1)) ** 2\n",
    "    y_var = np.var(y_predict, axis=1)\n",
    "\n",
    "    print(\"{0}: {1:.4f} (error) = {2:.4f} (bias^2) \"\n",
    "          \" + {3:.4f} (var) + {4:.4f} (noise)\".format(name,\n",
    "                                                      np.mean(y_error),\n",
    "                                                      np.mean(y_bias),\n",
    "                                                      np.mean(y_var),\n",
    "                                                      np.mean(y_noise)))\n",
    "    plt.figure(1, figsize=(12, 9))\n",
    "    # Plot figures\n",
    "    plt.subplot(2, n_estimators, n + 1)\n",
    "    plt.plot(X_test, f(X_test), \"b\", label=\"$f(x)$\")\n",
    "    plt.plot(X_train[0], y_train[0], \".b\", label=\"LS ~ $y = f(x)+noise$\")\n",
    "\n",
    "    for i in range(n_repeat):\n",
    "        if i == 0:\n",
    "            plt.plot(X_test, y_predict[:, i], \"r\", label=\"$\\^y(x)$\")\n",
    "        else:\n",
    "            plt.plot(X_test, y_predict[:, i], \"r\", alpha=0.05)\n",
    "\n",
    "    plt.plot(X_test, np.mean(y_predict, axis=1), \"c\",\n",
    "             label=\"$\\mathbb{E}_{LS} \\^y(x)$\")\n",
    "\n",
    "    plt.xlim([-5, 5])\n",
    "    plt.title(name)\n",
    "\n",
    "    if n == 0:\n",
    "        plt.legend(loc=\"upper left\", prop={\"size\": 11})\n",
    "\n",
    "    plt.subplot(2, n_estimators, n_estimators + n + 1)\n",
    "    plt.plot(X_test, y_error, \"r\", label=\"$error(x)$\")\n",
    "    plt.plot(X_test, y_bias, \"b\", label=\"$bias^2(x)$\"),\n",
    "    plt.plot(X_test, y_var, \"g\", label=\"$variance(x)$\"),\n",
    "    plt.plot(X_test, y_noise, \"c\", label=\"$noise(x)$\")\n",
    "\n",
    "    plt.xlim([-5, 5])\n",
    "    plt.ylim([0, 0.1])\n",
    "\n",
    "    if n == 0:\n",
    "        plt.legend(loc=\"upper left\", prop={\"size\": 11})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
